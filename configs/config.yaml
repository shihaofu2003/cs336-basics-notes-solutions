# conf/config.yaml

# 可以在命令行里用：
#   python train.py trainer.max_epochs=20 optimizer.lr=5e-4 model.d_model=768
# 来覆盖这里的默认值

seed: 2025

experiment_name: "baseline"

paths:
  root: ???
  data: ???
  output: ???

model:
  vocab_size: 10000
  context_length: 128
  num_layers: 2
  d_model: 128
  num_heads: 4
  d_ff: 256
  rope_theta: 2

lr_schedule:
    lr_max: 1e-2
    lr_min: 1e-5
    T_warmup: 1000
    T_anneal: 8000

clipping:
    max_l2_norm: 1.0
    eps: 1e-6

optimizer:
  lr: 1e-3
  betas: [0.9, 0.999]
  weight_decay: 0.0
  eps: 1e-8

trainer:
  epochs: 1
  batch_size: 4
  device: "cuda"   # 如果没有显卡，会在代码里自动 fallback 到 cpu
  iters_per_epoch: 10000      # 每个 epoch 做多少个梯度更新
  log_interval: 100          # 每多少步打印/记录一次
  save_interval: 2000

dataset:
  train_data_path: "${paths.data}/TinyStories/data_1000000_10000.npy"
  train_data_shape: 1000000

wandb:
  enabled: true           # 需要时设为 false 就不会初始化 wandb
  project: "cs336-basics" # 换成你自己的项目名
  entity: null            # 如果有 team，就写成 "my-team-name"，否则保持 null
  mode: "online"          # 还可以是 "offline" 或 "disabled"
  run_name: "${experiment_name}"   # Hydra 插值语法，等于 "cs336-basics"

# Hydra 自己的一些配置
hydra:
  run:
    # 每次单独运行，Hydra 会把当前工作目录切换到这个 dir
    dir: "experiments/${experiment_name}/${now:%Y-%m-%d_%H-%M-%S}"
  job:
    chdir: true
  sweep:
    dir: "multirun/${experiment_name}"
    subdir: "${now:%Y-%m-%d_%H-%M-%S}/${hydra.job.num}"
